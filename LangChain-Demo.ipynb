{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-R8VvOvoKced",
        "hXQgvPWpwKtE",
        "UajfEHuWPNmp",
        "EviOSJUZTAD5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "-R8VvOvoKced"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-openai"
      ],
      "metadata": {
        "id": "7gYlIfPeFcT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs_KfHH_FBwa",
        "outputId": "5e3cf9b5-48cb-4797-ca44-f7b300dd2e5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "VCJgKLpHHX9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()"
      ],
      "metadata": {
        "id": "gizfFqPrFFJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"What is the capital of USA\"\n",
        "\n",
        "response = llm.invoke(text)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_izKPfPHr68",
        "outputId": "3b6183eb-e5db-4e6e-883d-25202bcfcc7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The capital of the United States of America is Washington, D.C.')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "HUGGINGFACE_API_TOKEN = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URw_HL_0HzrQ",
        "outputId": "c0e3278e-9a2f-4f4e-e9e1-dda8a5dadb08"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_TOKEN\"] = HUGGINGFACE_API_TOKEN"
      ],
      "metadata": {
        "id": "lhH-ikDKH-1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "hf=HuggingFaceHub(repo_id=\"google/flan-t5-large\", huggingfacehub_api_token= HUGGINGFACE_API_TOKEN)"
      ],
      "metadata": {
        "id": "_S7_W0SWIF0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=hf.invoke(text)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG5jRryXIw_F",
        "outputId": "36a3d677-1a4e-434c-b35f-2c84cdcad07b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "washington dc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=hf.invoke(\"Can you tell me what deep learnin is?\")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs8D-FZBJgvO",
        "outputId": "bae709db-b671-4cee-9641-3f6fc1c34f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep learning is the study of human behavior through the use of machine learning techniques.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "text = \"Tell me a joke about Elon Musk\"\n",
        "\n",
        "messages = [HumanMessage(content=text)]"
      ],
      "metadata": {
        "id": "aDgO1dEIJvNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "chat_model = ChatOpenAI()"
      ],
      "metadata": {
        "id": "moiW8TAZJ1l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "JPdmKyVfJ483",
        "outputId": "0ca1211f-477f-420d-f8c9-23245b379003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nWhy did Elon Musk go to Mars?\\n\\nBecause he heard the Red Planet had good \"space-x\" coverage!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3J-wJQKJ9zn",
        "outputId": "ff4e65bf-9a99-4890-b503-f726f15a6b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Why did Elon Musk become a gardener?\\n\\nBecause he wanted to grow his own Tesla-cos!')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "hXQgvPWpwKtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-core langchain-openai"
      ],
      "metadata": {
        "id": "arXXUHUVyY0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()"
      ],
      "metadata": {
        "id": "V-C3MXstwMI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "8TFQ_ThO0mPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}?\")"
      ],
      "metadata": {
        "id": "Ju5VEcK7zPyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.format(topic=\"Bill Gates\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "N5D5GHTlzn42",
        "outputId": "337091d4-4c1b-4bd2-dadd-4ce1ff2ba257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a joke about Bill Gates?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI()"
      ],
      "metadata": {
        "id": "YTpyUPl1zsUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm\n",
        "\n",
        "chain.invoke({\"topic\": \"Elon Musk\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I67sRAvZ4vNZ",
        "outputId": "6c967e09-dc0b-4fc4-8a11-14b6ee45e304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Why did Elon Musk start a bakery?\\n\\nBecause he wanted to make a lot of dough!')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "YN40Sb7o93Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "YlFxASao97eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"topic\": \"Elon Musk\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gO3HnBvf9_0C",
        "outputId": "20ce116c-af18-4de0-c9dc-48af24c2a6a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why did Elon Musk become an astronaut?\\n\\nBecause he couldn't Elon-gate his love for space!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"topic\": \"Bill Gates\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "77nqdX4Y-bir",
        "outputId": "28e51efa-82e5-4493-bb1d-525a49bb1c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why did Bill Gates become a chef?\\n\\nBecause he wanted to create the perfect Microsoft \"soup\"!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "\n",
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\""
      ],
      "metadata": {
        "id": "74UQ_M9B-874"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_template = \"{text}\""
      ],
      "metadata": {
        "id": "QMaaHfT7Ap0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    (\"human\", human_template),\n",
        "])"
      ],
      "metadata": {
        "id": "uuh4Df8fAsRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt.format_messages(\n",
        "    input_language=\"English\", output_language=\"French\",\n",
        "    text=\"I love programming.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEtCdnmTAxd1",
        "outputId": "ed44f570-12ee-44d4-cf04-a4d9130ade8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful assistant that translates English to French.'),\n",
              " HumanMessage(content='I love programming.')]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains"
      ],
      "metadata": {
        "id": "UajfEHuWPNmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-openai"
      ],
      "metadata": {
        "id": "aveJpV4sPFKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIWIw_gnPd48",
        "outputId": "133e1f1f-e593-4acd-a67d-e5e6494b23bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "Z7hvKOjyPm90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} joke about {content}.\")"
      ],
      "metadata": {
        "id": "6HRZfYB-PpwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.format(adjective=\"funny\", content=\"Elon Musk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ADVptX9HP1LV",
        "outputId": "d4f608da-0d63-4001-d9f4-2e4df501a259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a funny joke about Elon Musk.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()"
      ],
      "metadata": {
        "id": "SQ_VwXIIQDvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(\"Give me a suggenstion for the main course for today's lunch.\")"
      ],
      "metadata": {
        "id": "MxwHVE75QFly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKeqZeg3QKT1",
        "outputId": "e2a2d628-fa75-4e0d-ef73-03fca1fc077a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How about a delicious grilled chicken with roasted vegetables and a side of mashed potatoes?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")"
      ],
      "metadata": {
        "id": "uuppe5TGQV_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI()"
      ],
      "metadata": {
        "id": "pBNjLm-yQY74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "mcBQUkPeQema"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "Eo_NRM6-QphY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"topic\": \"Elon Musk\"})"
      ],
      "metadata": {
        "id": "vnG95gCeQtKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlVRefoCQurL",
        "outputId": "353d0e10-47f7-4ceb-df5f-15ef4a1e2117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did Elon Musk start a comedy club on Mars?\n",
            "\n",
            "Because he wanted to create a space for \"out of this world\" laughter!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an English-French translator that return whatever the user says in French.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "9SulIBjpQ1DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "42jrGrL-RKlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"To be or not to be!\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Dw2_pCtvRLTD",
        "outputId": "b496b89a-1189-48ee-9477-72645f836ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Être ou ne pas être !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Chain"
      ],
      "metadata": {
        "id": "EviOSJUZTAD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-openai"
      ],
      "metadata": {
        "id": "x-_55jU1RNkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCFruB7mTPj4",
        "outputId": "2be76a3d-c761-4c1a-d51d-9503b945e4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "z1VXRgkKTWpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI()"
      ],
      "metadata": {
        "id": "RN8n4WZ2Tr7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are world class technical documentation writer.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "mBvVCOI9TxZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "4eXKODLyT-pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "Twn-dlYqULvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({\"input\": \"What is LangGraph?\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hok2w-_FUWzT",
        "outputId": "f8fb2eac-4b50-4b49-be40-3b30c2466fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph is a tool developed by OpenAI that enables users to visualize and explore the relationships between programming languages. It provides an interactive graph interface where users can navigate and understand the connections between various programming languages based on their similarities and influences.\n",
            "\n",
            "The graph in LangGraph represents programming languages as nodes, and the edges between them represent relationships such as being influenced by, being a successor of, or having similar features to another language. These relationships are based on data extracted from a vast amount of code repositories and other sources.\n",
            "\n",
            "With LangGraph, users can search for specific programming languages, explore their connections, and gain insights into the evolution of different programming paradigms and technologies. It can be a valuable tool for programmers, researchers, and anyone interested in understanding the landscape of programming languages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "oM-u8rAQVKwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://blog.langchain.dev/langgraph/\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "8AKxLfdGVOqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvYnSmxwVWdM",
        "outputId": "f6ff590c-0381-4aa1-f9d9-13c1ee8c2084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='\\n\\n\\nLangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n                LangChain Blog\\n        \\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph\\n\\n7 min read\\nJan 17, 2024\\n\\n\\n\\n\\n\\nTL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we\\'ve implemented already. We will then highlight a few of the common modifications to these runtimes we\\'ve heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We\\'ve invested heavily in the functionality for this with LangChain Expression Language. However, so far we\\'ve lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for handling of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by specifying them as graphs.FunctionalityAt it\\'s core, LangGraph exposes a pretty narrow interface on top of LangChain.StateGraphStateGraph is a class that represents the graph. You initialize this class by passing in a state definition. This state definition represents a central state object that is updated over time. This state is updated by nodes in the graph, which return operations to attributes of this state (in the form of a key-value store).The attributes of this state can be updated in two ways. First, an attribute could be overridden completely. This is useful if you want to nodes to return the new value of an attribute. Second, an attribute could be updated by adding to its value. This is useful if an attribute is a list of actions taken (or something similar) and you want nodes to return new actions taken (and have those automatically added to the attribute).You specify whether an attribute should be overridden or added to when creating the initial state definition. See an example in pseudocode below.from langgraph.graph import StateGraph\\nfrom typing import TypedDict, List, Annotated\\nimport Operator\\n\\n\\nclass State(TypedDict):\\n    input: str\\n    all_actions: Annotated[List[str], operator.add]\\n\\n\\ngraph = StateGraph(State)NodesAfter creating a StateGraph, you then add nodes with graph.add_node(name, value) syntax. The name parameter should be a string that we will use to refer to the node when adding edges. The value parameter should be either a function or LCEL runnable that will be called. This function/LCEL should accept a dictionary in the same form as the State object as input, and output a dictionary with keys of the State object to update.See an example in pseudocode below.graph.add_node(\"model\", model)\\ngraph.add_node(\"tools\", tool_executor)There is also a special END node that is used to represent the end of the graph. It is important that your cycles be able to end eventually!from langgraph.graph import ENDEdgesAfter adding nodes, you can then add edges to create the graph. There are a few types of edges.The Starting EdgeThis is the edge that connects the start of the graph to a particular node. This will make it so that that node is the first one called when input is passed to the graph. Pseudocode for that is:graph.set_entry_point(\"model\")Normal EdgesThese are edges where one node should ALWAYS be called after another. An example of this may be in the basic agent runtime, where we always want the model to be called after we call a tool.graph.add_edge(\"tools\", \"model\")Conditional EdgesThese are where a function (often powered by an LLM) is used to determine which node to go to first. To create this edge, you need to pass in three things:The upstream node: the output of this node will be looked at to determine what to do nextA function: this will be called to determine which node to call next. It should return a stringA mapping: this mapping will be used to map the output of the function in (2) to another node. The keys should be possible values that the function in (2) could return. The values should be names of nodes to go to if that value is returned.An example of this could be that after a model is called we either exit the graph and return to the user, or we call a tool - depending on what a user decides! See an example in pseudocode below:graph.add_conditional_edge(\\n    \"model\",\\n    should_continue,\\n    {\\n        \"end\": END,\\n        \"continue\": \"tools\"\\n    }\\n)CompileAfter we define our graph, we can compile it into a runnable! This simply takes the graph definition we\\'ve created so far an returns a runnable. This runnable exposes all the same method as LangChain runnables (.invoke, .stream, .astream_log, etc) allowing it to be called in the same manner as a chain.app = graph.compile()Agent ExecutorWe\\'ve recreated the canonical LangChain AgentExecutor with LangGraph. This will allow you to use existing LangChain agents, but allow you to more easily modify the internals of the AgentExecutor. The state of this graph by default contains concepts that should be familiar to you if you\\'ve used LangChain agents: input, chat_history, intermediate_steps (and agent_outcome to represent the most recent agent outcome)from typing import TypedDict, Annotated, List, Union\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.messages import BaseMessage\\nimport operator\\n\\n\\nclass AgentState(TypedDict):\\n   input: str\\n   chat_history: list[BaseMessage]\\n   agent_outcome: Union[AgentAction, AgentFinish, None]\\n   intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add] See this notebook for how to get startedChat Agent ExecutorOne common trend we\\'ve seen is that more and more models are \"chat\" models which operate on a list of messages. This models are often the ones equipped with things like function calling, which make agent-like experiences much more feasible. When working with these types of models, it is often intuitive to represent the state of an agent as a list of messages.As such, we\\'ve created an agent runtime that works with this state. The input is a list of messages, and nodes just simply add to this list of messages over time.from typing import TypedDict, Annotated, Sequence\\nimport operator\\nfrom langchain_core.messages import BaseMessage\\n\\n\\nclass AgentState(TypedDict):\\n    messages: Annotated[Sequence[BaseMessage], operator.add] See this notebook for how to get startedModificationsOne of the big benefits of LangGraph is that it exposes the logic of AgentExecutor in a far more natural and modifiable way. We\\'ve provided a few examples of modifications that we\\'ve heard requests for:Force Calling a ToolFor when you always want to make an agent call a tool first. For Agent Executor and Chat Agent Executor.Human-in-the-loopHow to add a human-in-the-loop step before calling tools. For Agent Executor and Chat Agent Executor.Managing Agent StepsFor adding custom logic on how to handle the intermediate steps an agent might take (useful for when there\\'s a lot of steps). For Agent Executor and Chat Agent Executor.Returning Output in a Specific FormatHow to make the agent return output in a specific format using function calling. Only for Chat Agent Executor.Dynamically Returning the Output of a Tool DirectlySometimes you may want to return the output of a tool directly. We provide an easy way to do with the return_direct parameter in LangChain. However, this makes it so that the output of a tool is ALWAYS returned directly. Sometimes, you may want to let the LLM choose whether to return the response directly or not. Only for Chat Agent Executor.Future WorkWe\\'re incredibly excited about the possibility of LangGraph enabling more custom and powerful agent runtimes. Some of the things we are looking to implement in the near future:More advanced agent runtimes from academia (LLM Compiler, plan-and-solve, etc)Stateful tools (allowing tools to modify some state)More controlled human-in-the-loop workflowsMulti-agent workflowsIf any of these resonate with you, please feel free to add an example notebook in the LangGraph repo, or reach out to us at hello@langchain.dev for more involved collaboration!\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            Â© LangChain Blog 2024 - Powered by Ghost\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'})]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "6-E714SWVaDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BER7HQHAVpZd",
        "outputId": "f8747842-99b6-46e5-cac8-8ed242cb2bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(docs)\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04Ym8fweVsQ2",
        "outputId": "a64bf49d-093b-4908-f41e-a0fb9c5f4e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n                LangChain Blog\\n        \\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph\\n\\n7 min read\\nJan 17, 2024', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
              " Document(page_content='TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we\\'ve implemented already. We will then highlight a few of the common modifications to these runtimes we\\'ve heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We\\'ve invested heavily in the functionality for this with LangChain Expression Language. However, so far we\\'ve lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for handling of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
              " Document(page_content=\"of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by specifying them as graphs.FunctionalityAt it's core, LangGraph exposes a pretty narrow interface on top of LangChain.StateGraphStateGraph is a class that represents the graph. You initialize this class by passing in a state definition. This state definition represents a central state object that is updated over time. This state is updated by nodes in the graph, which return operations to attributes of this state (in the form of a key-value store).The attributes of this state can be updated in two ways. First, an attribute could be overridden completely. This is useful if you want to nodes to return the new value of an attribute. Second, an attribute could be updated by adding to its value. This is useful if an attribute is a list of actions taken (or something similar) and you want nodes to return new actions taken (and have those automatically added to the attribute).You specify whether an attribute should be overridden or added to when creating the initial state definition. See an example in pseudocode below.from langgraph.graph import StateGraph\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
              " Document(page_content='from typing import TypedDict, List, Annotated\\nimport Operator', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
              " Document(page_content='class State(TypedDict):\\n    input: str\\n    all_actions: Annotated[List[str], operator.add]\\n\\n\\ngraph = StateGraph(State)NodesAfter creating a StateGraph, you then add nodes with graph.add_node(name, value) syntax. The name parameter should be a string that we will use to refer to the node when adding edges. The value parameter should be either a function or LCEL runnable that will be called. This function/LCEL should accept a dictionary in the same form as the State object as input, and output a dictionary with keys of the State object to update.See an example in pseudocode below.graph.add_node(\"model\", model)\\ngraph.add_node(\"tools\", tool_executor)There is also a special END node that is used to represent the end of the graph. It is important that your cycles be able to end eventually!from langgraph.graph import ENDEdgesAfter adding nodes, you can then add edges to create the graph. There are a few types of edges.The Starting EdgeThis is the edge that connects the start of the graph to a particular node. This will make it so that that node is the first one called when input is passed to the graph. Pseudocode for that is:graph.set_entry_point(\"model\")Normal EdgesThese are edges where one node should ALWAYS be called after another. An example of this may be in the basic agent runtime, where we always want the model to be called after we call a tool.graph.add_edge(\"tools\", \"model\")Conditional EdgesThese are where a function (often powered by an LLM) is used to determine which node to go to first. To create this edge, you need to pass in three things:The upstream node: the output of this node will be looked at to determine what to do nextA function: this will be called to determine which node to call next. It should return a stringA mapping: this mapping will be used to map the output of the function in (2) to another node. The keys should be possible values that the function in (2) could return. The values should be names of nodes to go to if that value is returned.An example of this could be that after a model is called we either exit the graph and return to the user, or we call a tool - depending on what a user decides! See an example in pseudocode below:graph.add_conditional_edge(\\n    \"model\",\\n    should_continue,\\n    {\\n        \"end\": END,\\n        \"continue\": \"tools\"\\n    }\\n)CompileAfter we define our graph, we can compile it into a runnable! This simply takes the graph definition we\\'ve created so far an returns a runnable. This runnable exposes all the same method as LangChain runnables (.invoke, .stream, .astream_log, etc) allowing it to be called in the same manner as a chain.app = graph.compile()Agent ExecutorWe\\'ve recreated the canonical LangChain AgentExecutor with LangGraph. This will allow you to use existing LangChain agents, but allow you to more easily modify the internals of the AgentExecutor. The state of this graph by default contains concepts that should be familiar to you if you\\'ve used LangChain agents: input, chat_history, intermediate_steps (and agent_outcome to represent the most recent agent outcome)from typing import TypedDict, Annotated, List, Union\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.messages import BaseMessage\\nimport operator', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
              " Document(page_content='class AgentState(TypedDict):\\n   input: str\\n   chat_history: list[BaseMessage]\\n   agent_outcome: Union[AgentAction, AgentFinish, None]\\n   intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add] See this notebook for how to get startedChat Agent ExecutorOne common trend we\\'ve seen is that more and more models are \"chat\" models which operate on a list of messages. This models are often the ones equipped with things like function calling, which make agent-like experiences much more feasible. When working with these types of models, it is often intuitive to represent the state of an agent as a list of messages.As such, we\\'ve created an agent runtime that works with this state. The input is a list of messages, and nodes just simply add to this list of messages over time.from typing import TypedDict, Annotated, Sequence\\nimport operator\\nfrom langchain_core.messages import BaseMessage\\n\\n\\nclass AgentState(TypedDict):\\n    messages: Annotated[Sequence[BaseMessage], operator.add] See this notebook for how to get startedModificationsOne of the big benefits of LangGraph is that it exposes the logic of AgentExecutor in a far more natural and modifiable way. We\\'ve provided a few examples of modifications that we\\'ve heard requests for:Force Calling a ToolFor when you always want to make an agent call a tool first. For Agent Executor and Chat Agent Executor.Human-in-the-loopHow to add a human-in-the-loop step before calling tools. For Agent Executor and Chat Agent Executor.Managing Agent StepsFor adding custom logic on how to handle the intermediate steps an agent might take (useful for when there\\'s a lot of steps). For Agent Executor and Chat Agent Executor.Returning Output in a Specific FormatHow to make the agent return output in a specific format using function calling. Only for Chat Agent Executor.Dynamically Returning the Output of a Tool DirectlySometimes you may want to return the output of a tool directly. We provide an easy way to do with the return_direct parameter in LangChain. However, this makes it so that the output of a tool is ALWAYS returned directly. Sometimes, you may want to let the LLM choose whether to return the response directly or not. Only for Chat Agent Executor.Future WorkWe\\'re incredibly excited about the possibility of LangGraph enabling more custom and powerful agent runtimes. Some of the things we are looking to implement in the near future:More advanced agent runtimes from academia (LLM Compiler, plan-and-solve, etc)Stateful tools (allowing tools to modify some state)More controlled human-in-the-loop workflowsMulti-agent workflowsIf any of these resonate with you, please feel free to add an example notebook in the LangGraph repo, or reach out to us at hello@langchain.dev for more involved collaboration!\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            Â© LangChain Blog 2024 - Powered by Ghost', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'})]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "7XeO__vkWL0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "\"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\")"
      ],
      "metadata": {
        "id": "FFIPdh4cWQAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "document_chain = create_stuff_documents_chain(model, prompt)"
      ],
      "metadata": {
        "id": "GW1OqCqjWeM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retriever = vector.as_retriever()"
      ],
      "metadata": {
        "id": "-6m2tyyHWkks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ],
      "metadata": {
        "id": "TqsGpVIJWvZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"What is LangGraph?\"})"
      ],
      "metadata": {
        "id": "OcP5OE4fWx9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJUiS_dJW0iu",
        "outputId": "aaafe88c-60f8-44d3-babe-b0b1fb932b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph is a module built on top of LangChain that enables the creation of cyclical graphs, particularly useful for agent runtimes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "my_doc = Document(\n",
        "    page_content=\"LangGraph is module built on top of LangChain to better enable creation of cyclical graphs\")\n",
        "\n",
        "document_chain.invoke({\n",
        "    \"input\": \"What is LangGraph?\",\n",
        "    \"context\": [my_doc]\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2VGLk5DmW19C",
        "outputId": "c859db84-c71d-4197-b3ab-e4575040e40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangGraph is a module built on top of LangChain.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Retrieval Chain"
      ],
      "metadata": {
        "id": "i81HCcangGox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-openai"
      ],
      "metadata": {
        "id": "9tOrRzrFgLYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()"
      ],
      "metadata": {
        "id": "i799jjIIg7Zg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f73486c-52e9-4e65-d1ab-67d184ca1271"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "QfhQeZqrg-LW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI()"
      ],
      "metadata": {
        "id": "eJbkZ9ODQD0y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://blog.langchain.dev/langgraph/\")"
      ],
      "metadata": {
        "id": "s70MjtEuQNCG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "7JgS6wUzQQXL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "0PFlZoAiQSp5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "obYQdZ1VQfCK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3XqHBAMRSbS",
        "outputId": "62c04bdf-2684-46ec-cc4f-f7a320e43e7f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vector = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "T77tvivmRTkL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector.as_retriever()"
      ],
      "metadata": {
        "id": "QC-65WWcRrvq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
        "])"
      ],
      "metadata": {
        "id": "VAiutoPIRvaT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "\n",
        "retriever_chain = create_history_aware_retriever(model, retriever, prompt)"
      ],
      "metadata": {
        "id": "VXW_rsz2ScQS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "chat_history = [\n",
        "    HumanMessage(content=\"Can I use LangGraph for agent runtimes?\"), AIMessage(content=\"Yes!\")]"
      ],
      "metadata": {
        "id": "Y_egXENtSgWr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me how\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4QG8Y8rSthX",
        "outputId": "c9ab7b74-497d-45d3-c6bd-85258f855c24"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we\\'ve implemented already. We will then highlight a few of the common modifications to these runtimes we\\'ve heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We\\'ve invested heavily in the functionality for this with LangChain Expression Language. However, so far we\\'ve lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for handling of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
              " Document(page_content=\"of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by specifying them as graphs.FunctionalityAt it's core, LangGraph exposes a pretty narrow interface on top of LangChain.StateGraphStateGraph is a class that represents the graph. You initialize this class by passing in a state definition. This state definition represents a central state object that is updated over time. This state is updated by nodes in the graph, which return operations to attributes of this state (in the form of a key-value store).The attributes of this state can be updated in two ways. First, an attribute could be overridden completely. This is useful if you want to nodes to return the new value of an attribute. Second, an attribute could be updated by adding to its value. This is useful if an attribute is a list of actions taken (or something similar) and you want nodes to return new actions taken (and have those automatically added to the attribute).You specify whether an attribute should be overridden or added to when creating the initial state definition. See an example in pseudocode below.from langgraph.graph import StateGraph\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
              " Document(page_content='LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n                LangChain Blog\\n        \\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph\\n\\n7 min read\\nJan 17, 2024', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
              " Document(page_content='class State(TypedDict):\\n    input: str\\n    all_actions: Annotated[List[str], operator.add]\\n\\n\\ngraph = StateGraph(State)NodesAfter creating a StateGraph, you then add nodes with graph.add_node(name, value) syntax. The name parameter should be a string that we will use to refer to the node when adding edges. The value parameter should be either a function or LCEL runnable that will be called. This function/LCEL should accept a dictionary in the same form as the State object as input, and output a dictionary with keys of the State object to update.See an example in pseudocode below.graph.add_node(\"model\", model)\\ngraph.add_node(\"tools\", tool_executor)There is also a special END node that is used to represent the end of the graph. It is important that your cycles be able to end eventually!from langgraph.graph import ENDEdgesAfter adding nodes, you can then add edges to create the graph. There are a few types of edges.The Starting EdgeThis is the edge that connects the start of the graph to a particular node. This will make it so that that node is the first one called when input is passed to the graph. Pseudocode for that is:graph.set_entry_point(\"model\")Normal EdgesThese are edges where one node should ALWAYS be called after another. An example of this may be in the basic agent runtime, where we always want the model to be called after we call a tool.graph.add_edge(\"tools\", \"model\")Conditional EdgesThese are where a function (often powered by an LLM) is used to determine which node to go to first. To create this edge, you need to pass in three things:The upstream node: the output of this node will be looked at to determine what to do nextA function: this will be called to determine which node to call next. It should return a stringA mapping: this mapping will be used to map the output of the function in (2) to another node. The keys should be possible values that the function in (2) could return. The values should be names of nodes to go to if that value is returned.An example of this could be that after a model is called we either exit the graph and return to the user, or we call a tool - depending on what a user decides! See an example in pseudocode below:graph.add_conditional_edge(\\n    \"model\",\\n    should_continue,\\n    {\\n        \"end\": END,\\n        \"continue\": \"tools\"\\n    }\\n)CompileAfter we define our graph, we can compile it into a runnable! This simply takes the graph definition we\\'ve created so far an returns a runnable. This runnable exposes all the same method as LangChain runnables (.invoke, .stream, .astream_log, etc) allowing it to be called in the same manner as a chain.app = graph.compile()Agent ExecutorWe\\'ve recreated the canonical LangChain AgentExecutor with LangGraph. This will allow you to use existing LangChain agents, but allow you to more easily modify the internals of the AgentExecutor. The state of this graph by default contains concepts that should be familiar to you if you\\'ve used LangChain agents: input, chat_history, intermediate_steps (and agent_outcome to represent the most recent agent outcome)from typing import TypedDict, Annotated, List, Union\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.messages import BaseMessage\\nimport operator', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'})]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "MIFUCwH3Sy7P"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "document_chain = create_stuff_documents_chain(model, prompt)"
      ],
      "metadata": {
        "id": "f65DOoRtTFsK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
      ],
      "metadata": {
        "id": "0Qx2_FTwTKKC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [\n",
        "    HumanMessage(content=\"Can I use LannGraph for agent runtimes?\"), AIMessage(content=\"Yes!\")]"
      ],
      "metadata": {
        "id": "0YbewBDSTOhp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me how\"\n",
        "})"
      ],
      "metadata": {
        "id": "RrdG_ckmTSIj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVE0WgWUTTFU",
        "outputId": "bda766f6-2df3-41eb-8055-f97d9452da19"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': [HumanMessage(content='Can I use LannGraph for agent runtimes?'),\n",
              "  AIMessage(content='Yes!')],\n",
              " 'input': 'Tell me how',\n",
              " 'context': [Document(page_content='TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we\\'ve implemented already. We will then highlight a few of the common modifications to these runtimes we\\'ve heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We\\'ve invested heavily in the functionality for this with LangChain Expression Language. However, so far we\\'ve lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for handling of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
              "  Document(page_content=\"of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by specifying them as graphs.FunctionalityAt it's core, LangGraph exposes a pretty narrow interface on top of LangChain.StateGraphStateGraph is a class that represents the graph. You initialize this class by passing in a state definition. This state definition represents a central state object that is updated over time. This state is updated by nodes in the graph, which return operations to attributes of this state (in the form of a key-value store).The attributes of this state can be updated in two ways. First, an attribute could be overridden completely. This is useful if you want to nodes to return the new value of an attribute. Second, an attribute could be updated by adding to its value. This is useful if an attribute is a list of actions taken (or something similar) and you want nodes to return new actions taken (and have those automatically added to the attribute).You specify whether an attribute should be overridden or added to when creating the initial state definition. See an example in pseudocode below.from langgraph.graph import StateGraph\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
              "  Document(page_content='LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n                LangChain Blog\\n        \\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph\\n\\n7 min read\\nJan 17, 2024', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
              "  Document(page_content='class State(TypedDict):\\n    input: str\\n    all_actions: Annotated[List[str], operator.add]\\n\\n\\ngraph = StateGraph(State)NodesAfter creating a StateGraph, you then add nodes with graph.add_node(name, value) syntax. The name parameter should be a string that we will use to refer to the node when adding edges. The value parameter should be either a function or LCEL runnable that will be called. This function/LCEL should accept a dictionary in the same form as the State object as input, and output a dictionary with keys of the State object to update.See an example in pseudocode below.graph.add_node(\"model\", model)\\ngraph.add_node(\"tools\", tool_executor)There is also a special END node that is used to represent the end of the graph. It is important that your cycles be able to end eventually!from langgraph.graph import ENDEdgesAfter adding nodes, you can then add edges to create the graph. There are a few types of edges.The Starting EdgeThis is the edge that connects the start of the graph to a particular node. This will make it so that that node is the first one called when input is passed to the graph. Pseudocode for that is:graph.set_entry_point(\"model\")Normal EdgesThese are edges where one node should ALWAYS be called after another. An example of this may be in the basic agent runtime, where we always want the model to be called after we call a tool.graph.add_edge(\"tools\", \"model\")Conditional EdgesThese are where a function (often powered by an LLM) is used to determine which node to go to first. To create this edge, you need to pass in three things:The upstream node: the output of this node will be looked at to determine what to do nextA function: this will be called to determine which node to call next. It should return a stringA mapping: this mapping will be used to map the output of the function in (2) to another node. The keys should be possible values that the function in (2) could return. The values should be names of nodes to go to if that value is returned.An example of this could be that after a model is called we either exit the graph and return to the user, or we call a tool - depending on what a user decides! See an example in pseudocode below:graph.add_conditional_edge(\\n    \"model\",\\n    should_continue,\\n    {\\n        \"end\": END,\\n        \"continue\": \"tools\"\\n    }\\n)CompileAfter we define our graph, we can compile it into a runnable! This simply takes the graph definition we\\'ve created so far an returns a runnable. This runnable exposes all the same method as LangChain runnables (.invoke, .stream, .astream_log, etc) allowing it to be called in the same manner as a chain.app = graph.compile()Agent ExecutorWe\\'ve recreated the canonical LangChain AgentExecutor with LangGraph. This will allow you to use existing LangChain agents, but allow you to more easily modify the internals of the AgentExecutor. The state of this graph by default contains concepts that should be familiar to you if you\\'ve used LangChain agents: input, chat_history, intermediate_steps (and agent_outcome to represent the most recent agent outcome)from typing import TypedDict, Annotated, List, Union\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.messages import BaseMessage\\nimport operator', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'})],\n",
              " 'answer': 'LangGraph is designed to enable the creation of agent runtimes. It is built on top of LangChain and is fully interoperable with the LangChain ecosystem. LangGraph allows you to easily create cyclical graphs, which are often needed for agent runtimes.\\n\\nTo use LangGraph for agent runtimes, you would first define a StateGraph, which represents the graph itself. You initialize the StateGraph by passing in a state definition, which represents a central state object that is updated over time. This state object is updated by nodes in the graph, which return operations to attributes of the state.\\n\\nYou can add nodes to the graph using the `graph.add_node(name, value)` syntax. The `name` parameter is a string that is used to refer to the node when adding edges, and the `value` parameter is a function or LCEL runnable that will be called. This function/LCEL should accept a dictionary in the same form as the State object as input and output a dictionary with keys of the State object to update.\\n\\nAfter adding nodes, you can add edges to create the graph. There are a few types of edges, including the starting edge, normal edges, and conditional edges. The starting edge connects the start of the graph to a particular node, making it the first node called when input is passed to the graph. Normal edges define the order in which nodes are called, while conditional edges use a function to determine which node to call next.\\n\\nOnce the graph is defined, you can compile it into a runnable using `graph.compile()`. This runnable can be invoked and called in the same manner as a LangChain runnable, allowing you to create and run agent runtimes.\\n\\nIn addition to using LangGraph for agent runtimes, you can also modify the internals of the LangChain AgentExecutor using LangGraph, giving you more control and flexibility in your agent runtime implementation.'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "IjlKX_u5TYpk",
        "outputId": "ebf9bfd7-098a-4f3e-b1fc-85b4aeb806ae"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangGraph is designed to enable the creation of agent runtimes. It is built on top of LangChain and is fully interoperable with the LangChain ecosystem. LangGraph allows you to easily create cyclical graphs, which are often needed for agent runtimes.\\n\\nTo use LangGraph for agent runtimes, you would first define a StateGraph, which represents the graph itself. You initialize the StateGraph by passing in a state definition, which represents a central state object that is updated over time. This state object is updated by nodes in the graph, which return operations to attributes of the state.\\n\\nYou can add nodes to the graph using the `graph.add_node(name, value)` syntax. The `name` parameter is a string that is used to refer to the node when adding edges, and the `value` parameter is a function or LCEL runnable that will be called. This function/LCEL should accept a dictionary in the same form as the State object as input and output a dictionary with keys of the State object to update.\\n\\nAfter adding nodes, you can add edges to create the graph. There are a few types of edges, including the starting edge, normal edges, and conditional edges. The starting edge connects the start of the graph to a particular node, making it the first node called when input is passed to the graph. Normal edges define the order in which nodes are called, while conditional edges use a function to determine which node to call next.\\n\\nOnce the graph is defined, you can compile it into a runnable using `graph.compile()`. This runnable can be invoked and called in the same manner as a LangChain runnable, allowing you to create and run agent runtimes.\\n\\nIn addition to using LangGraph for agent runtimes, you can also modify the internals of the LangChain AgentExecutor using LangGraph, giving you more control and flexibility in your agent runtime implementation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain Agent"
      ],
      "metadata": {
        "id": "9aXgp-d7TxhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.get_relevant_documents(\"what is LangGraph?\")[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BSmOwpWTcua",
        "outputId": "fcdc2aaf-12b1-4d63-be49-ed534e186f5e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we\\'ve implemented already. We will then highlight a few of the common modifications to these runtimes we\\'ve heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We\\'ve invested heavily in the functionality for this with LangChain Expression Language. However, so far we\\'ve lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for handling of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"langgraph_search\",\n",
        "    \"Search for information about LangGraph. For any questions about LangGraph, you must use this tool!\",\n",
        ")"
      ],
      "metadata": {
        "id": "muKnyL2LZNlL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tavily-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOB-9e43Zi3d",
        "outputId": "a370c0b4-e8de-4b8b-8a2a-8005bf9fb038"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.3.1-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tavily-python) (2.31.0)\n",
            "Requirement already satisfied: tiktoken==0.5.2 in /usr/local/lib/python3.10/dist-packages (from tavily-python) (0.5.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.2->tavily-python) (2023.12.25)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tavily-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tavily-python) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tavily-python) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tavily-python) (2024.2.2)\n",
            "Installing collected packages: tavily-python\n",
            "Successfully installed tavily-python-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TAVILY_API_KEY = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbDrHlFfa2iE",
        "outputId": "ce07621e-cc99-4791-bba5-ba30b6d72660"
      },
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY"
      ],
      "metadata": {
        "id": "SslAO3BMa8Bw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "search = TavilySearchResults()"
      ],
      "metadata": {
        "id": "EtQTKfRaZ5Fo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search.invoke(\"what is the weather in London\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v3PV7c7aE1N",
        "outputId": "a5021fd4-25b5-4b80-8a89-10f06185a528"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://weatherspark.com/h/y/45062/2024/Historical-Weather-during-2024-in-London-United-Kingdom',\n",
              "  'content': '2024 Weather History in London United Kingdom  Cloud Cover in 2024 in London Observed Weather in 2024 in London Hours of Daylight and Twilight in 2024 in London  London Temperature History 2024 Hourly Temperature in 2024 in London Compare London to another city:  Moon Rise, Set & Phases in 2024 in London Humidity Comfort Levels in 2024 in London Wind Speed in 2024 in London2024 Weather History in London United Kingdom. The data for this report comes from the London Heathrow Airport. See all nearby weather stations. Latest Report — 1:20 AM. Tue, Feb 6, 2024 15 min ago UTC 01:20 ... This report shows the past weather for London, providing a weather history for 2024. It features all historical weather data series ...'},\n",
              " {'url': 'https://www.metoffice.gov.uk/weather/forecast/gcpvn15h9',\n",
              "  'content': 'UK video forecast Environment Agency Updated: 22:06 (UTC) on Tue 5 Dec 2023  London & South East England weather forecast Tuesday 5 Dec - Saturday 9 Dec  Updated: 16:00 (UTC) on Tue 5 Dec 2023 UK long range weather forecast Sunday 10 Dec - Tuesday 19 Dec  Sun 10 Dec Sun 10 Dec Mon 11 Dec Mon 11 Dec Seven day forecast for City of LondonCity of London 7 day weather forecast including weather warnings, temperature, rain, wind, visibility, humidity and UV ... Sunset: 17:09. UV: Low; Pollution: Low; No pollen data. ... Updated: 04:00 (UTC) on Thu 8 Feb 2024. UK long range weather forecast'},\n",
              " {'url': 'https://www.bbc.com/weather/2643743',\n",
              "  'content': 'Forecast for London Latest forecast from BBC London This video can not be played  To play this video you need to enable JavaScript in your browser. Latest forecast for London Tonight  Accessibility links Search for a location London - Weather warnings issued Forecast - London Day by day forecast  Tomorrow a low pressure system will move in from the west. This will result in overcast skies and spells of heavy rain14-day weather forecast for London.'},\n",
              " {'url': 'https://metro.co.uk/2024/02/09/heavy-snow-batters-uk-forecasters-warn-worst-yet-come-20250088/',\n",
              "  'content': \"Snow falls across Britain as weather warnings predict worst to come  The Met Office has issued yellow warnings for snow and ice, starting at 6am for at least 24 hours.  Latest London news To get the latest news from the capital visit Metro.co.uk's London news hub.  All trains from Euston suspended due to incident at Bletchley pic.twitter.com/9JJxdZ6tIK Latest London newsThe Met Office has issued yellow warnings for snow and ice, starting at 6am for at least 24 hours. Further travel delays are expected and people are being urged to be 'cautious' on the roads ...\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [retriever_tool, search]"
      ],
      "metadata": {
        "id": "VFtKOAAQbXMf"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchainhub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOAhcig2bYyL",
        "outputId": "4e8d52a1-17e7-4b7d-8c6f-aa86c1eac910"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.31.0.20240125-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n",
            "Installing collected packages: types-requests, langchainhub\n",
            "Successfully installed langchainhub-0.1.14 types-requests-2.31.0.20240125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")"
      ],
      "metadata": {
        "id": "ASe4GKGBb9uG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFKuLFSjcBXS",
        "outputId": "5daab690-c906-447d-fee9-62b7f077d3b8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
              " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
              " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
              " MessagesPlaceholder(variable_name='agent_scratchpad')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_openai_functions_agent\n",
        "\n",
        "agent = create_openai_functions_agent(model, tools, prompt)"
      ],
      "metadata": {
        "id": "OlOdms_DcCx0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "GwWF0wQhcKFS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"hi!\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzFow-7hcMnQ",
        "outputId": "c86cc6e1-a74f-4f23-e82b-7285d29ade48"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mHello! How can I assist you today?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'hi!', 'output': 'Hello! How can I assist you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"what is the weather in New York?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgpqq7HncPOD",
        "outputId": "d913a788-82c8-4f68-9689-c5b8cb3277ca"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `tavily_search_results_json` with `{'query': 'weather in New York'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[{'url': 'https://weatherspark.com/h/y/23912/2024/Historical-Weather-during-2024-in-New-York-City-New-York-United-States', 'content': '2024 Weather History in New York City New York, United States  Daily Precipitation in 2024 in New York City Observed Weather in 2024 in New York City  New York City Temperature History 2024 Hourly Temperature in 2024 in New York City  Hours of Daylight and Twilight in 2024 in New York Citygentle breeze Wind Dir. 360 deg, N Cloud Cover Overcast 3,000 ft Mostly Cloudy 700 ft Mostly Cloudy 1,500 ft Raw: KEWR 021551Z 36010KT 7SM -RA BKN007 BKN015 OVC030 06/03 A2987 RMK AO2 SLP115 P0002 T00560028 This report shows the past weather for New York City, providing a weather history for 2024.'}, {'url': 'https://www.silive.com/weather/2024/01/new-york-winter-weather-outlook-for-february-a-look-at-temperature-precipitation.html', 'content': \"NY weather: Long-range February forecast for temperature, precipitation  The monthly temperature outlook for February 2024 as reported by the National\\xa0Weather\\xa0Service's Climate Prediction  outlooks on what the coming month can hold for New York in terms of precipitation and temperature.  Climate Prediction Center foresees a warmer, yet drier month for most of New York state.The monthly precipitation outlook for February 2024 shows New York, as reported by the National Weather Service's Climate Prediction Center, with equal chances of being above or below...\"}, {'url': 'https://www.news10.com/weather/02-09-2024-warm-for-now-but-changes-in-sight/', 'content': '02/09/2024: Warm For Now, But Changes in Sight by: Matt Mackie, Kevin Appleby Posted: Feb 9, 2024 / 05:40 AM EST  Updated: Feb 9, 2024 / 06:13 AM EST The Latest Storm Tracker Forecast from meteorologists Matt Mackie & Kevin Appleby:  Updated: Feb 9, 2024 / 06:13 AM EST by: Matt Mackie, Kevin Appleby Posted: Feb 9, 2024 / 05:40 AM EST  Trending on NEWS10 02/09/2024: Warm For Now, But Changes in Sight NYSP: CT man arrested for alleged assault in WiltonNew York News; North Country; ... 02/09/2024: Warm For Now, But Changes in Sight by: Matt Mackie, ... Get the Android Weather app from Google Play. Stay Connected.'}, {'url': 'https://www.bloomberg.com/news/articles/2024-02-09/el-nino-threatens-us-east-coast-with-late-february-snow-polar-vortex', 'content': 'A pedestrian in Central Park during a snow storm in New York\\xa0on\\xa0Jan. 16.  Extreme Weather: Snow Storms Threaten US Northeast in February After Spring-Like Warmth  Weather & Science Snow Storms Threaten US Northeast in February After Spring-Like Warmth Featured CityLab  BloombergSnow Storms Threaten US Northeast in February After Spring-Like Warmth. Polar vortex is at risk of weakening, sending cold air south. A pedestrian in Central Park during a snow storm in New York ...'}]\u001b[0m\u001b[32;1m\u001b[1;3mI found some information about the weather in New York. Here are some sources you can check for more details:\n",
            "\n",
            "1. [WeatherSpark](https://weatherspark.com/h/y/23912/2024/Historical-Weather-during-2024-in-New-York-City-New-York-United-States): This website provides historical weather data for New York City in 2024, including daily precipitation and observed weather.\n",
            "\n",
            "2. [Staten Island Advance](https://www.silive.com/weather/2024/01/new-york-winter-weather-outlook-for-february-a-look-at-temperature-precipitation.html): This article discusses the long-range February forecast for temperature and precipitation in New York. The Climate Prediction Center predicts a warmer yet drier month for most of the state.\n",
            "\n",
            "3. [NEWS10](https://www.news10.com/weather/02-09-2024-warm-for-now-but-changes-in-sight/): This news article provides an update on the weather forecast for New York City on February 9, 2024. It mentions that there may be changes in the weather.\n",
            "\n",
            "4. [Bloomberg](https://www.bloomberg.com/news/articles/2024-02-09/el-nino-threatens-us-east-coast-with-late-february-snow-polar-vortex): This article discusses the threat of snow storms in the US Northeast, including New York, in late February. It mentions the risk of the polar vortex weakening and sending cold air south.\n",
            "\n",
            "Please note that the information provided may vary depending on the source and the specific date you are looking for. It is recommended to check the latest weather updates from reliable sources for the most accurate and up-to-date information.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is the weather in New York?',\n",
              " 'output': 'I found some information about the weather in New York. Here are some sources you can check for more details:\\n\\n1. [WeatherSpark](https://weatherspark.com/h/y/23912/2024/Historical-Weather-during-2024-in-New-York-City-New-York-United-States): This website provides historical weather data for New York City in 2024, including daily precipitation and observed weather.\\n\\n2. [Staten Island Advance](https://www.silive.com/weather/2024/01/new-york-winter-weather-outlook-for-february-a-look-at-temperature-precipitation.html): This article discusses the long-range February forecast for temperature and precipitation in New York. The Climate Prediction Center predicts a warmer yet drier month for most of the state.\\n\\n3. [NEWS10](https://www.news10.com/weather/02-09-2024-warm-for-now-but-changes-in-sight/): This news article provides an update on the weather forecast for New York City on February 9, 2024. It mentions that there may be changes in the weather.\\n\\n4. [Bloomberg](https://www.bloomberg.com/news/articles/2024-02-09/el-nino-threatens-us-east-coast-with-late-february-snow-polar-vortex): This article discusses the threat of snow storms in the US Northeast, including New York, in late February. It mentions the risk of the polar vortex weakening and sending cold air south.\\n\\nPlease note that the information provided may vary depending on the source and the specific date you are looking for. It is recommended to check the latest weather updates from reliable sources for the most accurate and up-to-date information.'}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "chat_history = [HumanMessage(content=\"Can I use LangGraph for agent runtimes?\"), AIMessage(content=\"Yes!\")]\n",
        "\n",
        "agent_executor.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me how\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OImZ9b5CcUN9",
        "outputId": "8aa85cfa-67bb-49d8-b927-2f2466dd12c5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `langgraph_search` with `{'query': 'LangGraph for agent runtimes'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mTL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we've implemented already. We will then highlight a few of the common modifications to these runtimes we've heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We've invested heavily in the functionality for this with LangChain Expression Language. However, so far we've lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it's often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we've seen in practice as we've worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for handling of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by\n",
            "\n",
            "of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by specifying them as graphs.FunctionalityAt it's core, LangGraph exposes a pretty narrow interface on top of LangChain.StateGraphStateGraph is a class that represents the graph. You initialize this class by passing in a state definition. This state definition represents a central state object that is updated over time. This state is updated by nodes in the graph, which return operations to attributes of this state (in the form of a key-value store).The attributes of this state can be updated in two ways. First, an attribute could be overridden completely. This is useful if you want to nodes to return the new value of an attribute. Second, an attribute could be updated by adding to its value. This is useful if an attribute is a list of actions taken (or something similar) and you want nodes to return new actions taken (and have those automatically added to the attribute).You specify whether an attribute should be overridden or added to when creating the initial state definition. See an example in pseudocode below.from langgraph.graph import StateGraph\n",
            "\n",
            "LangGraph\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                LangChain Blog\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Home\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "By LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Release Notes\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "GitHub\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Docs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Case Studies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign in\n",
            "Subscribe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangGraph\n",
            "\n",
            "7 min read\n",
            "Jan 17, 2024\n",
            "\n",
            "class State(TypedDict):\n",
            "    input: str\n",
            "    all_actions: Annotated[List[str], operator.add]\n",
            "\n",
            "\n",
            "graph = StateGraph(State)NodesAfter creating a StateGraph, you then add nodes with graph.add_node(name, value) syntax. The name parameter should be a string that we will use to refer to the node when adding edges. The value parameter should be either a function or LCEL runnable that will be called. This function/LCEL should accept a dictionary in the same form as the State object as input, and output a dictionary with keys of the State object to update.See an example in pseudocode below.graph.add_node(\"model\", model)\n",
            "graph.add_node(\"tools\", tool_executor)There is also a special END node that is used to represent the end of the graph. It is important that your cycles be able to end eventually!from langgraph.graph import ENDEdgesAfter adding nodes, you can then add edges to create the graph. There are a few types of edges.The Starting EdgeThis is the edge that connects the start of the graph to a particular node. This will make it so that that node is the first one called when input is passed to the graph. Pseudocode for that is:graph.set_entry_point(\"model\")Normal EdgesThese are edges where one node should ALWAYS be called after another. An example of this may be in the basic agent runtime, where we always want the model to be called after we call a tool.graph.add_edge(\"tools\", \"model\")Conditional EdgesThese are where a function (often powered by an LLM) is used to determine which node to go to first. To create this edge, you need to pass in three things:The upstream node: the output of this node will be looked at to determine what to do nextA function: this will be called to determine which node to call next. It should return a stringA mapping: this mapping will be used to map the output of the function in (2) to another node. The keys should be possible values that the function in (2) could return. The values should be names of nodes to go to if that value is returned.An example of this could be that after a model is called we either exit the graph and return to the user, or we call a tool - depending on what a user decides! See an example in pseudocode below:graph.add_conditional_edge(\n",
            "    \"model\",\n",
            "    should_continue,\n",
            "    {\n",
            "        \"end\": END,\n",
            "        \"continue\": \"tools\"\n",
            "    }\n",
            ")CompileAfter we define our graph, we can compile it into a runnable! This simply takes the graph definition we've created so far an returns a runnable. This runnable exposes all the same method as LangChain runnables (.invoke, .stream, .astream_log, etc) allowing it to be called in the same manner as a chain.app = graph.compile()Agent ExecutorWe've recreated the canonical LangChain AgentExecutor with LangGraph. This will allow you to use existing LangChain agents, but allow you to more easily modify the internals of the AgentExecutor. The state of this graph by default contains concepts that should be familiar to you if you've used LangChain agents: input, chat_history, intermediate_steps (and agent_outcome to represent the most recent agent outcome)from typing import TypedDict, Annotated, List, Union\n",
            "from langchain_core.agents import AgentAction, AgentFinish\n",
            "from langchain_core.messages import BaseMessage\n",
            "import operator\u001b[0m\u001b[32;1m\u001b[1;3mLangGraph is a module built on top of LangChain that enables the creation of cyclical graphs, which are often needed for agent runtimes. It provides an easy way to create state machines and agent loops.\n",
            "\n",
            "Here is a brief overview of how to use LangGraph for agent runtimes:\n",
            "\n",
            "1. Start by defining the state of your agent runtime using the StateGraph class. This state represents a central object that is updated over time. You can specify the attributes of this state and how they should be updated.\n",
            "\n",
            "2. Add nodes to the graph using the `add_node` method. Nodes can be functions or LangChain Expression Language (LCEL) runnables that accept the state as input and return updated values for the state attributes.\n",
            "\n",
            "3. Create edges to connect the nodes in the graph. There are different types of edges you can use:\n",
            "   - Starting Edge: Connects the start of the graph to a specific node.\n",
            "   - Normal Edges: Specify the order in which nodes should be called.\n",
            "   - Conditional Edges: Use a function to determine which node to call next based on the output of a previous node.\n",
            "\n",
            "4. Compile the graph into a runnable using the `compile` method. This will create a runnable object that can be invoked and called like a LangChain chain.\n",
            "\n",
            "Additionally, you can use the AgentExecutor class provided by LangGraph to recreate the canonical LangChain AgentExecutor. This allows you to use existing LangChain agents and modify the internals of the AgentExecutor.\n",
            "\n",
            "For more detailed information and examples, you can refer to the LangGraph documentation and examples.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': [HumanMessage(content='Can I use LangGraph for agent runtimes?'),\n",
              "  AIMessage(content='Yes!')],\n",
              " 'input': 'Tell me how',\n",
              " 'output': 'LangGraph is a module built on top of LangChain that enables the creation of cyclical graphs, which are often needed for agent runtimes. It provides an easy way to create state machines and agent loops.\\n\\nHere is a brief overview of how to use LangGraph for agent runtimes:\\n\\n1. Start by defining the state of your agent runtime using the StateGraph class. This state represents a central object that is updated over time. You can specify the attributes of this state and how they should be updated.\\n\\n2. Add nodes to the graph using the `add_node` method. Nodes can be functions or LangChain Expression Language (LCEL) runnables that accept the state as input and return updated values for the state attributes.\\n\\n3. Create edges to connect the nodes in the graph. There are different types of edges you can use:\\n   - Starting Edge: Connects the start of the graph to a specific node.\\n   - Normal Edges: Specify the order in which nodes should be called.\\n   - Conditional Edges: Use a function to determine which node to call next based on the output of a previous node.\\n\\n4. Compile the graph into a runnable using the `compile` method. This will create a runnable object that can be invoked and called like a LangChain chain.\\n\\nAdditionally, you can use the AgentExecutor class provided by LangGraph to recreate the canonical LangChain AgentExecutor. This allows you to use existing LangChain agents and modify the internals of the AgentExecutor.\\n\\nFor more detailed information and examples, you can refer to the LangGraph documentation and examples.'}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "109PSWxncfF3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}